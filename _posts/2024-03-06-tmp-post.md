---
layout: post
read_time: true
show_date: true
title: This is a post
date: 2024-03-06
description: (for testing)
img: posts/20240306/abstract_1.jpeg
tags: [coding, tesing]
category: testing
author: Jia-Chang
github: JiaChangGit/network-packet-classification/tree/rangeTypeRule/docs
mathjax: yes # leave empty or erase to prevent the mathjax javascript from loading
toc: yes # leave empty or erase for no TOC
---
用來測試ing [back-to-basics](./back-to-basics.html)

![test symbol0](./assets/img/posts/20240306/test-symb0.png)
![test symbol1](./assets/img/posts/20240306/test-symb1.png)
### Adam
[NCKU-CSIE](https://www.csie.ncku.edu.tw/zh-hant)

<p>math test0:</p>
<p style="text-align:center">\(<br>
\begin{align}<br>
\begin{split}<br>
m_t &amp;= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\<br>
v_t &amp;= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2<br>
\end{split}<br>
\end{align}<br>
\)</p>
<p>\(m_t\) and \(v_t\) are estimates of ...</p>
<p>math test1:</p>
<p style="text-align:center">\(<br>
\begin{align}<br>
\begin{split}<br>
\hat{m}_t &amp;= \dfrac{m_t}{1 - \beta^t_1} \\<br>
\hat{v}_t &amp;= \dfrac{v_t}{1 - \beta^t_2} \end{split}<br>
\end{align}<br>
\)</p>

<p style="text-align:center">\(\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\).</p>
<p>The authors propose defaults of 0.9 for \(\beta_1\), 0.999 for \(\beta_2\), and \(10^{-8}\) for \(\epsilon\).</p>

```python
# decaying averages of past gradients
self.v["dW" + str(i)] = ((c.BETA1
                        * self.v["dW" + str(i)])
                        + ((1 - c.BETA1)
                        * np.array(self.gradients[i])
                        ))
self.v["db" + str(i)] = ((c.BETA1
                        * self.v["db" + str(i)])
                        + ((1 - c.BETA1)
                        * np.array(self.bias_gradients[i])
                        ))

# decaying averages of past squared gradients
self.s["dW" + str(i)] = ((c.BETA2
                        * self.s["dW"+str(i)])
                        + ((1 - c.BETA2)
                        * (np.square(np.array(self.gradients[i])))
                         ))
self.s["db" + str(i)] = ((c.BETA2
                        * self.s["db" + str(i)])
                        + ((1 - c.BETA2)
                        * (np.square(np.array(
                                         self.bias_gradients[i])))
                         ))

if c.ADAM_BIAS_Correction:
    # bias-corrected first and second moment estimates
    self.v["dW" + str(i)] = self.v["dW" + str(i)]
                          / (1 - (c.BETA1 ** true_epoch))
    self.v["db" + str(i)] = self.v["db" + str(i)]
                          / (1 - (c.BETA1 ** true_epoch))
    self.s["dW" + str(i)] = self.s["dW" + str(i)]
                          / (1 - (c.BETA2 ** true_epoch))
    self.s["db" + str(i)] = self.s["db" + str(i)]
                          / (1 - (c.BETA2 ** true_epoch))

# apply to weights and biases
weight_col -= ((eta * (self.v["dW" + str(i)]
                      / (np.sqrt(self.s["dW" + str(i)])
                      + c.EPSILON))))
self.bias[i] -= ((eta * (self.v["db" + str(i)]
                        / (np.sqrt(self.s["db" + str(i)])
                        + c.EPSILON))))
```
